# main.py (ì¢…í•© ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€ ë²„ì „)

from fastapi import FastAPI, Request
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional

import feedparser
import re 
from googletrans import Translator
import requests 
from collections import Counter # ì¶œì²˜ ë¹ˆë„ìˆ˜ ê³„ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€

# --- (ì´ì „ ì½”ë“œì™€ ë™ì¼: NewsArticle ëª¨ë¸ ì •ì˜, Translator ê°ì²´ ì´ˆê¸°í™”) ---
class NewsArticle(BaseModel):
    title: str
    url: str
    source: str
    summary: Optional[str] = None 

translator = Translator()


# --- 2. RSS í”¼ë“œ íŒŒì‹± ë° ë²ˆì—­ ë¡œì§ êµ¬í˜„ (ë³€ê²½ ì—†ìŒ) ---
def parse_rss_feed() -> List[NewsArticle]:
    # ... (ì´ì „ parse_rss_feed í•¨ìˆ˜ ë‚´ìš© ì „ì²´ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•©ë‹ˆë‹¤.) ...
    feed_url = "https://techcrunch.com/category/artificial-intelligence/feed/" 
    news_list: List[NewsArticle] = []
    
    clean_html = re.compile('<.*?>') 

    try:
        feed = feedparser.parse(feed_url)
        
        for entry in feed.entries[:10]:
            original_title = entry.title
            try:
                translated_title = translator.translate(original_title, dest='ko').text
            except Exception:
                translated_title = original_title + " (ë²ˆì—­ ì‹¤íŒ¨)"

            raw_summary = entry.summary if hasattr(entry, 'summary') else ""
            clean_summary = re.sub(clean_html, '', raw_summary).strip()
            
            truncated_summary_ko = None
            if clean_summary:
                try:
                    translated_text = translator.translate(clean_summary, dest='ko').text
                    truncated_summary_ko = translated_text[:200]
                    if len(translated_text) > 200:
                         truncated_summary_ko += "..."
                except Exception:
                    truncated_summary_ko = clean_summary[:200] + "... (ë²ˆì—­ ì‹¤íŒ¨)"

            news_list.append(NewsArticle(
                title=translated_title,
                url=entry.link,
                source=entry.author if hasattr(entry, 'author') else feed.feed.title,
                summary=truncated_summary_ko
            ))
            
        return news_list

    except Exception as e:
        print(f"ğŸš¨ RSS í”¼ë“œ íŒŒì‹± ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return [
            NewsArticle(title=f"ğŸš¨ RSS í”¼ë“œ ì˜¤ë¥˜: {e}", url="#", source="ì˜¤ë¥˜", summary="ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."),
        ]


# --- ğŸŒŸ 4. ìƒˆë¡œìš´ ì¢…í•© ë¶„ì„ ë¡œì§ êµ¬í˜„ ğŸŒŸ ---
def analyze_news_data(news: List[NewsArticle]) -> str:
    """
    10ê°œ ê¸°ì‚¬ì˜ ì¶œì²˜ ë° ì£¼ì œ ë™í–¥ì„ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë¶„ì„ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if not news or news[0].url == '#':
        return "ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í•˜ì—¬ ì¢…í•© ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    sources = [article.source for article in news if article.source and article.source != 'ì˜¤ë¥˜']
    source_counts = Counter(sources)
    
    total_articles = len(news)
    
    # 1. ì¶œì²˜ ë¹ˆë„ ë¶„ì„
    if source_counts:
        most_common_source, count = source_counts.most_common(1)[0]
        source_analysis = f"ì´ {total_articles}ê°œì˜ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©°, ì£¼ìš” ì¶œì²˜ëŠ” '{most_common_source}' (ì´ {count}ê±´)ì…ë‹ˆë‹¤."
    else:
        source_analysis = f"ì´ {total_articles}ê°œì˜ ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë‚˜, ì¶œì²˜ ì •ë³´ê°€ ë¶ˆë¶„ëª…í•©ë‹ˆë‹¤."
        
    # 2. ì£¼ì œ ë™í–¥ ë¶„ì„ (í‚¤ì›Œë“œ ì¶”ì¶œ ê¸°ë°˜ì˜ ê°€ìƒ ë¶„ì„)
    # ì‹¤ì œ ìì—°ì–´ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šê³  ì œëª©ì— í¬í•¨ëœ íŠ¹ì • í‚¤ì›Œë“œ ë¹ˆë„ë¥¼ ì´ìš©í•´ ê°€ìƒ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    keywords = ['ëª¨ë¸', 'ìŠ¤íƒ€íŠ¸ì—…', 'ë°˜ë„ì²´', 'íˆ¬ì', 'ê·œì œ', 'GPT', 'ëŸ¬ë‹', 'ë¡œë´‡', 'ì¹©']
    title_text = " ".join([article.title for article in news])
    
    keyword_summary = {}
    for kw in keywords:
        kw_count = title_text.count(kw)
        if kw_count > 0:
            keyword_summary[kw] = kw_count
            
    if keyword_summary:
        sorted_keywords = sorted(keyword_summary.items(), key=lambda item: item[1], reverse=True)
        top_keywords = ", ".join([f"{k} ({v}íšŒ)" for k, v in sorted_keywords[:3]])
        trend_analysis = f"í˜„ì¬ ì£¼ìš” ê´€ì‹¬ì‚¬ëŠ” {top_keywords} ë“±ìœ¼ë¡œ, ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì˜ ìƒìš©í™”ì™€ ê´€ë ¨ íˆ¬ì, ê·¸ë¦¬ê³  í•˜ë“œì›¨ì–´(ë°˜ë„ì²´/ì¹©) ê¸°ìˆ ì— ëŒ€í•œ ë™í–¥ì´ ë‘ë“œëŸ¬ì§‘ë‹ˆë‹¤."
    else:
        trend_analysis = "í˜„ì¬ ê¸°ì‚¬ ì œëª©ë§Œìœ¼ë¡œëŠ” ëª…í™•í•œ ì£¼ìš” ë™í–¥ í‚¤ì›Œë“œë¥¼ íŒŒì•…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤."
        
    # ìµœì¢… ë¶„ì„ ê²°í•©
    final_analysis = (
        "**AI ë‰´ìŠ¤ ì¢…í•© ë¶„ì„ ê²°ê³¼**\n\n"
        f"1. **ë°ì´í„° ê°œìš”:** {source_analysis}\n"
        f"2. **ê¸°ìˆ  ë™í–¥:** {trend_analysis} \n\n"
        "ì´ ë¶„ì„ì€ 10ê°œ ê¸°ì‚¬ì˜ ì œëª©ê³¼ ì¶œì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹¨ìˆœ í†µê³„ì´ë©°, ì‹¤ì œ ë‚´ìš© ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ NLP(ìì—°ì–´ ì²˜ë¦¬) ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. "
        "ì „ë°˜ì ìœ¼ë¡œ AI ê¸°ìˆ ì˜ ìƒì—…ì  ì ìš©ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ê°€ í™œë°œí•˜ê²Œ ë³´ë„ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    return final_analysis

# --- 3. FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ë° ë¼ìš°íŠ¸ ì •ì˜ ---
app = FastAPI(
    title="ì¸ê³µì§€ëŠ¥ ë²ˆì—­ RSS ë‰´ìŠ¤ í”¼ë“œ ì•±",
    description="FastAPI, feedparser, googletransë¥¼ ì´ìš©í•´ ì¸ê³µì§€ëŠ¥ ë‰´ìŠ¤ë¥¼ ë²ˆì—­ ë° ê²Œì‹œí•©ë‹ˆë‹¤."
)

templates = Jinja2Templates(directory="templates")

@app.get("/", summary="ë‰´ìŠ¤ ì›¹ í˜ì´ì§€ í‘œì‹œ")
async def news_webpage(request: Request):
    news = parse_rss_feed() 
    
    # ğŸŒŸ ì¶”ê°€: ì¢…í•© ë¶„ì„ ë‚´ìš© ìƒì„±
    analysis_text = analyze_news_data(news)

    return templates.TemplateResponse(
        "index.html", 
        {"request": request, "news": news, "title": "ì¸ê³µì§€ëŠ¥ (AI) ë²ˆì—­ ë‰´ìŠ¤", "analysis": analysis_text} # í…œí”Œë¦¿ì— ë¶„ì„ ë‚´ìš© ì „ë‹¬
    )

@app.get("/api/news", response_model=List[NewsArticle], summary="ë‰´ìŠ¤ ë°ì´í„° (JSON) ë°˜í™˜")
async def get_latest_news_api():
    return parse_rss_feed()